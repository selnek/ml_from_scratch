{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c207293c-dccf-4965-a638-c4dce2c97818",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is dimensionality reduction technique used to reduce the complexity of high-dimensional data by transforming the data into a lower-dimensional space while retaining as much of the original variability as possible. It is an unsupervised learning algorithm that identifies patterns in the data and re-expresses the data in terms of a smaller number of variables called principal components.\n",
    "\n",
    "The key idea behind PCA is to identify the directions in the data that contain the most variation, and to project the data onto these directions to create a new set of variables that capture as much of the original information as possible. The first principal component is the direction that explains the most variation in the data, and each subsequent principal component explains as much of the remaining variation as possible.\n",
    "\n",
    "PCA can be summarized in the following steps:\n",
    "\n",
    "1. Gather and preprocess the data.\n",
    "2. Standardize the data to ensure that all variables are on the same scale.\n",
    "3. Compute the covariance matrix of the standardized data.\n",
    "4. Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "5. Sort the eigenvalues in descending order, and select the top k eigenvectors corresponding to the k largest eigenvalues.\n",
    "6. Transform the data by multiplying it by the selected eigenvectors to obtain the principal components.\n",
    "7. Use the transformed data for analysis or visualization.\n",
    "\n",
    "PCA is widely used in applications such as image processing, finance, and biology. It provides a powerful and flexible approach for dimensionality reduction, and can help identify patterns and relationships in high-dimensional data. PCA can also be used as a preprocessing step for other machine learning algorithms to reduce the dimensionality of the input data and improve their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e833925-4766-4b5b-a8ed-78152baf417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # mean centering\n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        X = X - self.mean\n",
    "        \n",
    "        # covariance\n",
    "        cov = np.cov(X.T)\n",
    "        \n",
    "        # eigenvectors, eigenvalues \n",
    "        eigenvectors, eigenvalues = np.linalg.eig(cov)\n",
    "        eigenvectors = eigenvectors.T\n",
    "        \n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "        \n",
    "        self.components = eigenvectors[:self.n_components]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # data projection\n",
    "        X = X - self.mean\n",
    "        return np.dot(X, self.components.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59b811-13c0-4ace-bf69-c891a5331282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "pca = PCA(2)\n",
    "pca.fit(X)\n",
    "X_projected = pca.transform(X)\n",
    "\n",
    "\n",
    "x1 = X_projected[:, 0]\n",
    "x2 = X_projected[:, 1]\n",
    "\n",
    "plt.scatter(\n",
    "    x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee768f-5572-4bc3-9c99-20766fe3e902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
