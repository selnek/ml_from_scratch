{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e82168e-fbf0-4677-9d72-ebad0a2e4c82",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic algorithm used for classification and is based on Bayes' theorem. It assumes that the features are independent of each other, hence the name \"naive\".\n",
    "\n",
    "In Naive Bayes, we calculate the probability of a data point belonging to each possible class based on the probability of each feature given that class. Then, we choose the class with the highest probability as the predicted class for the data point.\n",
    "\n",
    "Naive Bayes is commonly used for text classification, spam filtering, sentiment analysis, and recommendation systems. It is computationally efficient and requires a relatively small amount of training data. However, its assumption of independence between features may not hold in some cases, which can affect its accuracy.\n",
    "\n",
    "The formula for Naive Bayes can be derived from Bayes' theorem, which states that the probability of a hypothesis (H) given the observed evidence (E) is proportional to the product of the prior probability of the hypothesis (P(H)) and the likelihood of the evidence given the hypothesis (P(E|H)), divided by the marginal likelihood of the evidence (P(E)):\n",
    "\n",
    "P(H|E) = P(H) * P(E|H) / P(E)\n",
    "\n",
    "In the context of Naive Bayes, we use this formula to calculate the probability of a data point belonging to a specific class (H) given its features (E). We assume that the features are conditionally independent given the class, so we can calculate the likelihood as the product of the probabilities of each feature given the class:\n",
    "\n",
    "P(E|H) = P(feature1|H) * P(feature2|H) * ... * P(featureN|H)\n",
    "\n",
    "Then, we use Bayes' theorem to calculate the probability of each class given the data point's features and choose the class with the highest probability:\n",
    "\n",
    "P(H|E) = P(H) * P(E|H) / P(E)\n",
    "\n",
    "where P(E) is a normalization constant that ensures the sum of the probabilities of all possible classes adds up to 1.\n",
    "\n",
    "In practice, we estimate the probabilities and likelihoods from the training data, and use them to classify new data points based on their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc87bc7d-13f6-4606-beab-73e4feacf99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        \n",
    "        #calculate mean, variance, prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._prob_density_fn(idx, x)))\n",
    "            posterior += prior\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def _prob_density_fn(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "350da50a-a121-453e-9313-93744e46677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baaf02f-ee9a-4cc7-912b-8b86af1bf822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
