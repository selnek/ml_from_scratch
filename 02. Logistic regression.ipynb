{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5187582-1cfa-4b7a-9c5c-78c8d9f0ac7b",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a popular machine learning algorithm used for binary classification tasks, where the goal is to predict the probability of a binary outcome (e.g., 0 or 1, yes or no). It is a type of regression analysis that models the relationship between the input features and the probability of the output variable.\n",
    "\n",
    "The logistic regression algorithm works by estimating the probability of the output variable using a logistic function, also known as the sigmoid function. The sigmoid function maps any input value to a value between 0 and 1, which can be interpreted as the probability of the output variable being 1.\n",
    "\n",
    "The loss function used in logistic regression is the cross-entropy loss, which measures the difference between the predicted probability and the true probability. The goal of the algorithm is to minimize the cross-entropy loss by adjusting the weights and bias of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60535e8-9451-4f12-bc75-1248b0dea4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _sigmoid(x):\n",
    "    sig = 1 / (1 + np.exp(-x))\n",
    "    return sig\n",
    "    \n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            predictions = _sigmoid(linear_pred)\n",
    "            \n",
    "            dw = (1 / n_samples) * np.dot(X.T, (predictions - y))\n",
    "            db = (1 / n_samples) * np.sum(predictions - y)\n",
    "            \n",
    "            self.weights = self.weights - self.lr * dw\n",
    "            self.bias = self.bias - self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = _sigmoid(linear_pred)\n",
    "        class_pred = [0 if y < 0.5 else 1 for y in y_pred]\n",
    "        return class_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f74183-86a1-4776-a4d0-5a09458dbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test)/len(y_test)\n",
    "\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2efcd5-5774-4573-b231-4ed1f989472b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
