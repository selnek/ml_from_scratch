{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ee9f5a-1946-49df-9b3a-6828c06d7174",
   "metadata": {},
   "source": [
    "## K-means\n",
    "\n",
    "K-means is a unsupervised machine learning algorithm used for clustering tasks. The goal of the algorithm is to partition a set of n data points into k clusters, where each data point belongs to the cluster with the nearest mean.\n",
    "\n",
    "K-means is an iterative algorithm that aims to minimize the sum of squared distances between the data points and their respective cluster centroids. The algorithm starts with an initial set of k centroids, and then iteratively refines the centroids by re-assigning the data points to the nearest cluster and recalculating the centroid of each cluster.\n",
    "\n",
    "K-means is a simple yet effective algorithm that can be used for a wide range of clustering tasks. It is widely used in applications such as customer segmentation, image processing, and anomaly detection. However, the algorithm can be sensitive to the initial choice of centroids, and may converge to a sub-optimal solution. To mitigate this, multiple runs of the algorithm with different initializations can be performed, and the solution with the lowest sum of squared distances can be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef840f03-5466-48c4-9e74-05497ba4081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x2 - x1) ** 2))\n",
    "\n",
    "    \n",
    "class KMeans():\n",
    "    \n",
    "    \n",
    "    def __init__(self, K=3, n_iters=100, plot_steps=False):\n",
    "        self.K = K\n",
    "        self.n_iters = n_iters\n",
    "        self.plot_steps = plot_steps\n",
    "        \n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "        self.centroids = []\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # initialize\n",
    "        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
    "        \n",
    "        # optimize\n",
    "        for _ in range(self.n_iters):\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "            \n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "            \n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "                \n",
    "        # classify\n",
    "        return self._get_cluster_labels(self.clusters)\n",
    "    \n",
    "    \n",
    "    def _create_clusters(self, centroids):\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "           \n",
    "        \n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
    "        closest_idx = np.argmin(distances)\n",
    "        return closest_idx\n",
    "    \n",
    "    \n",
    "    def _get_centroids(self, clusters):\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean  = np.mean(self.X[cluster], axis=0)\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        return centroids\n",
    "        \n",
    "        \n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n",
    "        return sum(distances) == 0\n",
    "    \n",
    "    \n",
    "    def _get_cluster_labels(self, clusters):\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_idx in clusters:\n",
    "                labels[sample_idx] = cluster_idx\n",
    "                \n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        for i, index in enumerate(self.clusters):\n",
    "            point = self.X[index].T\n",
    "            ax.scatter(*point)\n",
    "\n",
    "        for point in self.centroids:\n",
    "            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71487d15-32f0-4e3c-96dd-475a16970b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(centers=3, n_samples=500, n_features=2, shuffle=True, random_state=40)\n",
    "print(X.shape)\n",
    "\n",
    "clusters = len(np.unique(y))\n",
    "print(clusters)\n",
    "\n",
    "k = KMeans(K=clusters, n_iters=150, plot_steps=True)\n",
    "y_pred = k.predict(X)\n",
    "\n",
    "k.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
